import { DEFAULT_MODELS } from "@/app/constant";
import { useAccessStore, useAppConfig, useChatStore } from "@/app/store";
import { ChatOptions, LLMApi, LLMModel, LLMUsage } from "../api";
import { GoogleGenAI } from "@google/genai";
import {
  isWebSearchModel,
  getModelCapabilitiesWithCustomConfig,
} from "@/app/config/model-capabilities";
import { getMessageTextContent, getMessageImages } from "@/app/utils";

export class GoogleGenAIApi implements LLMApi {
  private client: GoogleGenAI | null = null;

  constructor() {
    this.initializeClient();
  }

  private initializeClient() {
    const accessStore = useAccessStore.getState();

    // è·å– API é…ç½®
    const apiKey = accessStore.googleApiKey;
    const customUrl = accessStore.useCustomConfig
      ? accessStore.googleUrl
      : null;

    if (!apiKey) {
      console.warn("[GoogleGenAI] No API key provided");
      return;
    }

    try {
      const clientConfig: any = { apiKey };

      // å¦‚æœæœ‰è‡ªå®šä¹‰ URLï¼Œæ·»åŠ åˆ°é…ç½®ä¸­
      if (customUrl) {
        clientConfig.baseUrl = customUrl;
      }

      this.client = new GoogleGenAI(clientConfig);
    } catch (error) {
      console.error("[GoogleGenAI] âŒ Failed to initialize client:", error);
    }
  }

  async chat(options: ChatOptions): Promise<void> {
    if (!this.client) {
      this.initializeClient();
      if (!this.client) {
        throw new Error("Failed to initialize Google GenAI client");
      }
    }

    const messages = options.messages
      .filter((v) => v.role === "user" || v.role === "assistant") // åªä¿ç•™æœ‰æ•ˆè§’è‰²
      .map((v) => ({
        role: v.role === "assistant" ? "model" : "user", // ç¡®ä¿åªæœ‰ user å’Œ model è§’è‰²
        parts: [
          {
            text: getMessageTextContent(v),
          },
          ...getMessageImages(v).map((image) => ({
            inlineData: {
              mimeType: image.split(";")[0].split(":")[1],
              data: image.split(",")[1],
            },
          })),
        ],
      }));

    const modelConfig = {
      ...useAppConfig.getState().modelConfig,
      ...useChatStore.getState().currentSession().mask.modelConfig,
      ...{
        model: options.config.model,
      },
    };

    // æ£€æŸ¥æ˜¯å¦å¯ç”¨æœç´¢åŠŸèƒ½
    const session = useChatStore.getState().currentSession();
    const enableWebSearch = session.searchEnabled ?? false;
    const isSearchModel = isWebSearchModel(options.config.model);

    // æ£€æŸ¥æ¨¡å‹èƒ½åŠ›
    const modelCapabilities = getModelCapabilitiesWithCustomConfig(
      options.config.model,
    );

    // é…ç½®å·¥å…·
    const tools: any[] = [];
    if (enableWebSearch && isSearchModel) {
      if (options.config.model.includes("gemini-1.5")) {
        // Gemini 1.5 ä½¿ç”¨ googleSearchRetrieval
        tools.push({
          googleSearchRetrieval: {
            dynamicRetrievalConfig: {
              mode: "MODE_DYNAMIC",
              dynamicThreshold: 0.7,
            },
          },
        });
      } else {
        // Gemini 2.x ä½¿ç”¨ googleSearch
        tools.push({
          googleSearch: {},
        });
      }
    }

    // å®‰å…¨è®¾ç½®
    const safetySettings = [
      {
        category: "HARM_CATEGORY_HARASSMENT",
        threshold: "BLOCK_ONLY_HIGH",
      },
      {
        category: "HARM_CATEGORY_HATE_SPEECH",
        threshold: "BLOCK_ONLY_HIGH",
      },
      {
        category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
        threshold: "BLOCK_ONLY_HIGH",
      },
      {
        category: "HARM_CATEGORY_DANGEROUS_CONTENT",
        threshold: "BLOCK_ONLY_HIGH",
      },
    ];

    // ç”Ÿæˆé…ç½®
    const config: any = {
      temperature: modelConfig.temperature,
      maxOutputTokens: modelConfig.max_tokens,
      topP: modelConfig.top_p,
      safetySettings,
      // å¦‚æœå¯ç”¨æœç´¢ï¼Œä½¿ç”¨æ¨èçš„æ¸©åº¦è®¾ç½®
      ...(enableWebSearch &&
        tools.length > 0 && {
          temperature: 1.0,
        }),
      ...(tools.length > 0 && { tools }),
    };

    // å¦‚æœæ¨¡å‹å…·æœ‰æ¨ç†èƒ½åŠ›ä¸”æ˜¯Geminiç±»å‹ï¼Œæ·»åŠ æ€è€ƒé…ç½®
    if (
      modelCapabilities.reasoning &&
      modelCapabilities.thinkingType === "gemini"
    ) {
      const thinkingBudget = modelConfig.thinkingBudget ?? -1;

      // æ„å»ºthinkingé…ç½®
      const thinkingConfig: any = {
        includeThoughts: true, // åŒ…å«æ€è€ƒå†…å®¹
      };

      // åªæœ‰å½“thinkingBudgetä¸ä¸ºundefinedæ—¶æ‰æ·»åŠ 
      if (thinkingBudget !== undefined) {
        thinkingConfig.thinkingBudget = thinkingBudget;
      }

      config.thinkingConfig = thinkingConfig;
    }

    try {
      // ä½¿ç”¨ models.generateContentStream è¿›è¡Œæµå¼ç”Ÿæˆ
      const response = await this.client.models.generateContentStream({
        model: options.config.model,
        contents: messages,
        config,
      });

      let responseText = "";
      let isInThinkingMode = false;

      for await (const chunk of response) {
        // å¤„ç†æ€è€ƒå†…å®¹å’Œæ™®é€šå†…å®¹
        if (chunk.candidates && chunk.candidates.length > 0) {
          const candidate = chunk.candidates[0];
          if (candidate.content && candidate.content.parts) {
            for (const part of candidate.content.parts) {
              if (part.thought && part.text) {
                // è¿™æ˜¯æ€è€ƒå†…å®¹ - ç›´æ¥ä½¿ç”¨ <think> æ ‡ç­¾åŒ…è£…

                // å¦‚æœåˆšè¿›å…¥æ€è€ƒæ¨¡å¼ï¼Œæ·»åŠ å¼€å§‹æ ‡ç­¾
                if (!isInThinkingMode) {
                  isInThinkingMode = true;
                  if (responseText.length > 0) {
                    responseText += "\n";
                  }
                  responseText += "<think>\n" + part.text;
                } else {
                  // ç»§ç»­æ·»åŠ æ€è€ƒå†…å®¹
                  responseText += part.text;
                }

                console.log(
                  "[GoogleGenAI] ğŸ§  Current responseText after thought:",
                  responseText.substring(responseText.length - 100),
                );
                options.onUpdate?.(responseText, part.text);
              } else if (part.text && !part.thought) {
                // è¿™æ˜¯æ™®é€šå†…å®¹

                // å¦‚æœä»æ€è€ƒæ¨¡å¼åˆ‡æ¢åˆ°æ™®é€šæ¨¡å¼ï¼Œæ·»åŠ ç»“æŸæ ‡ç­¾å’Œåˆ†éš”ç¬¦
                if (isInThinkingMode) {
                  isInThinkingMode = false;
                  responseText += "\n</think>\n\n";
                }

                responseText += part.text;
                options.onUpdate?.(responseText, part.text);
              }
            }
          }
        }

        // å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ° candidatesï¼Œä½¿ç”¨ chunk.text
        if (!chunk.candidates && chunk.text) {
          responseText += chunk.text;
          options.onUpdate?.(responseText, chunk.text);
        }
      }

      // å¦‚æœæµç»“æŸæ—¶è¿˜åœ¨æ€è€ƒæ¨¡å¼ï¼Œæ·»åŠ ç»“æŸæ ‡ç­¾
      if (isInThinkingMode) {
        responseText += "\n</think>";
      }
      // åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ Response å¯¹è±¡
      const mockResponse = new Response(responseText, { status: 200 });
      options.onFinish(responseText, mockResponse);
    } catch (error) {
      console.error("[GoogleGenAI] âŒ Chat error:", error);
      options.onError?.(error as Error);
    }
  }

  usage(): Promise<LLMUsage> {
    throw new Error("Method not implemented.");
  }

  async models(): Promise<LLMModel[]> {
    return DEFAULT_MODELS.filter((m) => m.name.startsWith("gemini"));
  }

  async speech(_options: any): Promise<ArrayBuffer> {
    throw new Error("Speech generation not implemented for GoogleGenAI");
  }
}
